{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classificação da HbA1c com Modelos Supervisionados\n",
        "\n",
        "Este notebook executa o pipeline completo de preparação de dados, otimização de hiperparâmetros e avaliação de modelos para predizer a classe A1c (0 = não diabético, 1 = diabético) a partir de dados hematológicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visão Geral do Notebook\n",
        "\n",
        "1. Importação de bibliotecas e definição de métricas\n",
        "2. Funções utilitárias para carregar, limpar e transformar os dados\n",
        "3. Construção do conjunto de treino/teste preservando o desbalanceamento\n",
        "4. Definição do pré-processamento com `ColumnTransformer`\n",
        "5. Busca de hiperparâmetros via `RandomizedSearchCV` (Stratified K-Fold)\n",
        "6. Seleção do melhor modelo com base no F2-Score\n",
        "7. Avaliação final no conjunto de teste usando F2-Score e AUC-PR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    fbeta_score,\n",
        "    make_scorer,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "set_config(display=\"diagram\")\n",
        "\n",
        "try:\n",
        "    import pymysql\n",
        "except ImportError:\n",
        "    pymysql = None\n",
        "    print(\"Aviso: pymysql não está disponível; será usada apenas a simulação de dados.\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    lgb = None\n",
        "    print(\"Aviso: lightgbm não está instalado. O modelo LightGBM será ignorado.\")\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except ImportError:\n",
        "    XGBClassifier = None\n",
        "    print(\"Aviso: xgboost não está instalado. O modelo XGBoost será ignorado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fetch_data_in_batches(query, MYSQL_HOST, MYSQL_PORT, MYSQL_USERNAME, MYSQL_PASSWORD, DB_NAME, batch_size=10000):\n",
        "    \"\"\"Busca dados de um banco MySQL em lotes\"\"\"\n",
        "    if pymysql is None:\n",
        "        print(\"fetch_data_in_batches: pymysql indisponível. Retornando None para acionar simulação.\")\n",
        "        return None\n",
        "\n",
        "    connection = None\n",
        "    df_list = []\n",
        "\n",
        "    try:\n",
        "        connection = pymysql.connect(\n",
        "            host=MYSQL_HOST,\n",
        "            port=MYSQL_PORT,\n",
        "            user=MYSQL_USERNAME,\n",
        "            password=MYSQL_PASSWORD,\n",
        "            db=DB_NAME,\n",
        "            charset='utf8mb4',\n",
        "            cursorclass=pymysql.cursors.DictCursor\n",
        "        )\n",
        "        print(\"Conexão estabelecida com sucesso!\")\n",
        "\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(query)\n",
        "            while True:\n",
        "                results = cursor.fetchmany(batch_size)\n",
        "                if not results:\n",
        "                    break\n",
        "                df_list.append(pd.DataFrame(results))\n",
        "\n",
        "        if df_list:\n",
        "            df = pd.concat(df_list, ignore_index=True)\n",
        "        else:\n",
        "            df = pd.DataFrame()\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante a conexão/busca: {e}. Retornando None para usar dados simulados.\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        if connection:\n",
        "            connection.close()\n",
        "            print(\"Conexão fechada.\")\n",
        "\n",
        "\n",
        "def clean_data(df):\n",
        "    df = df.dropna()\n\n",
        "    df = df[(df != '').all(axis=1)]\n",
        "    return df\n",
        "\n",
        "\n",
        "def fix_data_types(df):\n",
        "    float_cols = ['Leucócitos', 'Mielócitos', 'Metamielócitos', 'Bastões', 'Segmentados',\n",
        "                  'Eosinófilos', 'Basófilos', 'Linfócitos', 'Linfócitos Atípicos',\n",
        "                  'Monócitos', 'Plasmócitos', 'Blastos', 'Eritrócitos', 'Hemoglobina',\n",
        "                  'Hematócrito', 'HCM', 'CHCM', 'RDW', 'Plaquetas', 'MVP',\n",
        "                  'Promielócitos', 'A1C']\n",
        "    int_cols = ['Classe A1c', 'Classe A1c2']\n",
        "\n",
        "    for col in float_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
        "\n",
        "    for col in int_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            df[col] = df[col].fillna(-1).astype(int)\n",
        "\n",
        "    df = df.dropna(subset=[c for c in float_cols if c in df.columns] + [c for c in int_cols if c in df.columns])\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_unusual_variables(df):\n",
        "    cols_to_drop = [\n",
        "        'CodigoOs', 'Data Nascimento', 'Classe Idade', 'HCM_1', 'GME', 'G',\n",
        "        'Paciente', 'Data Cadastro', 'Data Cadastro Date', 'Data Nascimento Data',\n",
        "        'Classe A1c', 'A1C'\n",
        "    ]\n",
        "    existing = [col for col in cols_to_drop if col in df.columns]\n",
        "    return df.drop(columns=existing, errors='ignore')\n",
        "\n",
        "\n",
        "def remove_outliers(df):\n",
        "    thresholds = {\n",
        "        'Leucócitos': 200000,\n",
        "        'Mielócitos': 2000,\n",
        "        'Metamielócitos': 4000,\n",
        "        'Segmentados': 50000,\n",
        "        'Eosinófilos': 25000,\n",
        "        'Basófilos': 400,\n",
        "        'Linfócitos': 100000,\n",
        "        'Linfócitos Atípicos': 1700,\n",
        "        'Monócitos': 10000,\n",
        "        'Plasmócitos': 1000,\n",
        "        'Blastos': 50000,\n",
        "        'Eritrócitos': 8.8,\n",
        "        'Hemoglobina': (3, None),\n",
        "        'HCM': 140,\n",
        "        'CHCM': 38,\n",
        "        'MVP': (3, None),\n",
        "        'Plaquetas': 1300\n",
        "    }\n",
        "    for col, limit in thresholds.items():\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        if isinstance(limit, tuple):\n",
        "            lower, upper = limit\n",
        "            if lower is not None:\n",
        "                df = df[df[col] >= lower]\n",
        "            if upper is not None:\n",
        "                df = df[df[col] <= upper]\n",
        "        else:\n",
        "            df = df[df[col] <= limit]\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_train_test_subsets(df):\n",
        "    X = df.drop('Classe A1c2', axis=1)\n",
        "    y = df['Classe A1c2']\n",
        "    return train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "\n",
        "def simulate_data():\n",
        "    np.random.seed(42)\n",
        "    n_total_samples = 11623\n",
        "    numerical_features = ['Idade', 'Leucócitos', 'Mielócitos', 'Metamielócitos', 'Bastões', 'Segmentados',\n",
        "                          'Eosinófilos', 'Basófilos', 'Linfócitos', 'Linfócitos Atípicos', 'Monócitos',\n",
        "                          'Plasmócitos', 'Blastos', 'Eritrócitos', 'Hemoglobina', 'Hematócrito', 'HCM',\n",
        "                          'CHCM', 'RDW', 'Plaquetas', 'MVP', 'Promielócitos', 'NPxD']\n",
        "    X_data = pd.DataFrame(np.random.rand(n_total_samples, len(numerical_features)), columns=numerical_features)\n",
        "    X_data['Sexo'] = np.random.choice(['F', 'M'], n_total_samples)\n",
        "    X_data['NPxD'] = np.random.randint(0, 2, n_total_samples)\n",
        "\n",
        "    y_data = pd.Series(np.zeros(n_total_samples, dtype=int))\n",
        "    minority_size = int(n_total_samples * 0.16)\n",
        "    y_data.iloc[np.random.choice(n_total_samples, minority_size, replace=False)] = 1\n",
        "    y_data.name = 'Classe A1c2'\n",
        "\n",
        "    print(f\"Dados simulados gerados: {n_total_samples} amostras.\")\n",
        "\n",
        "    return train_test_split(\n",
        "        X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
        "    )\n",
        "\n",
        "\n",
        "def get_train_test_data(query, MYSQL_HOST, MYSQL_PORT, MYSQL_USERNAME, MYSQL_PASSWORD, DB_NAME):\n",
        "    df = fetch_data_in_batches(query, MYSQL_HOST, MYSQL_PORT, MYSQL_USERNAME, MYSQL_PASSWORD, DB_NAME)\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        print(\"Aviso: dados reais não disponíveis. Usando simulação.\")\n",
        "        return simulate_data()\n",
        "\n",
        "    df = clean_data(df)\n",
        "    df = fix_data_types(df)\n",
        "    df = remove_unusual_variables(df)\n",
        "    df = remove_outliers(df)\n",
        "\n",
        "    return create_train_test_subsets(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "MYSQL_HOST = \"localhost\"\n",
        "MYSQL_PORT = 3306\n",
        "MYSQL_USERNAME = \"user\"\n",
        "MYSQL_PASSWORD = \"password\"\n",
        "DB_NAME = \"db_name\"\n",
        "query = \"SELECT * FROM dados_hematologicos\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_train_test_data(\n",
        "    query, MYSQL_HOST, MYSQL_PORT, MYSQL_USERNAME, MYSQL_PASSWORD, DB_NAME\n",
        ")\n",
        "\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "print(f\"Shape X_test: {X_test.shape}\")\n",
        "print(\"\\nDistribuição da variável alvo (treino):\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "categorical_features = ['Sexo'] if 'Sexo' in X_train.columns else []\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "class_weights = y_train.value_counts(normalize=True).to_dict()\n",
        "minority_weight = class_weights[0] / class_weights[1]\n",
        "class_weight_dict = {0: 1, 1: minority_weight}\n",
        "print(\"Pesos de classe calculados:\", class_weight_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "f2_scorer = make_scorer(fbeta_score, beta=2, average='binary')\n",
        "auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
        "scoring = {'f2_score': f2_scorer, 'auc_pr': auc_pr_scorer}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_search_space = {}\n",
        "\n",
        "model_search_space['LogisticRegression'] = {\n",
        "    'estimator': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', LogisticRegression(\n",
        "            random_state=42,\n",
        "            solver='liblinear',\n",
        "            class_weight=class_weight_dict,\n",
        "            max_iter=2000\n",
        "        ))\n",
        "    ]),\n",
        "    'param_distributions': {\n",
        "        'model__C': np.logspace(-3, 3, 20),\n",
        "        'model__penalty': ['l1', 'l2']\n",
        "    }\n",
        "}\n",
        "\n",
        "model_search_space['RandomForest'] = {\n",
        "    'estimator': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', RandomForestClassifier(\n",
        "            random_state=42,\n",
        "            class_weight=class_weight_dict,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ]),\n",
        "    'param_distributions': {\n",
        "        'model__n_estimators': [200, 400, 600, 800],\n",
        "        'model__max_depth': [10, 20, 30, None],\n",
        "        'model__min_samples_split': [2, 5, 10],\n",
        "        'model__min_samples_leaf': [1, 3, 5]\n",
        "    }\n",
        "}\n",
        "\n",
        "model_search_space['HistGradientBoosting'] = {\n",
        "    'estimator': Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', HistGradientBoostingClassifier(\n",
        "            random_state=42,\n",
        "            class_weight=class_weight_dict,\n",
        "        ))\n",
        "    ]),\n",
        "    'param_distributions': {\n",
        "        'model__learning_rate': np.linspace(0.01, 0.3, 20),\n",
        "        'model__max_depth': [3, 5, 7, None],\n",
        "        'model__max_leaf_nodes': [15, 31, 63],\n",
        "        'model__min_samples_leaf': [10, 20, 30]\n",
        "    }\n",
        "}\n",
        "\n",
        "if lgb is not None:\n",
        "    model_search_space['LightGBM'] = {\n",
        "        'estimator': Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', lgb.LGBMClassifier(\n",
        "                random_state=42,\n",
        "                class_weight=class_weight_dict,\n",
        "                n_jobs=-1,\n",
        "                verbose=-1\n",
        "            ))\n",
        "        ]),\n",
        "        'param_distributions': {\n",
        "            'model__n_estimators': [300, 600, 900],\n",
        "            'model__learning_rate': np.linspace(0.01, 0.2, 10),\n",
        "            'model__num_leaves': [31, 63, 127],\n",
        "            'model__max_depth': [-1, 10, 20],\n",
        "            'model__subsample': [0.7, 0.85, 1.0],\n",
        "            'model__colsample_bytree': [0.6, 0.8, 1.0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "if XGBClassifier is not None:\n",
        "    model_search_space['XGBoost'] = {\n",
        "        'estimator': Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', XGBClassifier(\n",
        "                random_state=42,\n",
        "                eval_metric='logloss',\n",
        "                n_jobs=-1,\n",
        "                tree_method='hist',\n",
        "                scale_pos_weight=class_weight_dict[1]\n",
        "            ))\n",
        "        ]),\n",
        "        'param_distributions': {\n",
        "            'model__n_estimators': [300, 600, 900],\n",
        "            'model__max_depth': [3, 5, 7],\n",
        "            'model__learning_rate': np.linspace(0.01, 0.3, 10),\n",
        "            'model__subsample': [0.6, 0.8, 1.0],\n",
        "            'model__colsample_bytree': [0.6, 0.8, 1.0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(f\"Modelos configurados: {list(model_search_space.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "n_iter_search = 25\n",
        "random_state = 42\n",
        "\n",
        "search_results = []\n",
        "best_model = None\n",
        "best_model_name = None\n",
        "best_score = -np.inf\n",
        "\n",
        "for name, config in model_search_space.items():\n",
        "    print(f\"\\n=== Otimizando {name} ===\")\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=config['estimator'],\n",
        "        param_distributions=config['param_distributions'],\n",
        "        n_iter=n_iter_search,\n",
        "        scoring=scoring,\n",
        "        refit='f2_score',\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        random_state=random_state,\n",
        "        verbose=0\n",
        "    )\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    best_idx = search.best_index_\n",
        "    mean_f2 = search.cv_results_['mean_test_f2_score'][best_idx]\n",
        "    mean_auc_pr = search.cv_results_['mean_test_auc_pr'][best_idx]\n",
        "\n",
        "    search_results.append({\n",
        "        'model': name,\n",
        "        'best_params': search.best_params_,\n",
        "        'mean_f2': mean_f2,\n",
        "        'mean_auc_pr': mean_auc_pr\n",
        "    })\n",
        "\n",
        "    print(f\"Melhor F2-Score (CV): {mean_f2:.4f}\")\n",
        "    print(f\"Melhor AUC-PR (CV): {mean_auc_pr:.4f}\")\n",
        "\n",
        "    if mean_f2 > best_score:\n",
        "        best_score = mean_f2\n",
        "        best_model = search.best_estimator_\n",
        "        best_model_name = name\n",
        "        best_search = search\n",
        "\n",
        "results_df = pd.DataFrame(search_results).sort_values(by='mean_f2', ascending=False)\n",
        "print(\"\\nResumo dos resultados (ordenado por F2-Score):\")\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(f\"\\nModelo selecionado: {best_model_name}\")\n",
        "print(f\"F2-Score médio (CV): {best_score:.4f}\")\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "f2_test = fbeta_score(y_test, y_pred, beta=2)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "auc_pr_test = auc(recall, precision)\n",
        "auc_roc_test = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(\"\\nMatriz de Confusão:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nMétricas no conjunto de teste:\")\n",
        "print(f\"F2-Score: {f2_test:.4f}\")\n",
        "print(f\"AUC-PR: {auc_pr_test:.4f}\")\n",
        "print(f\"AUC-ROC: {auc_roc_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Curva PR (AUC = {auc_pr_test:.3f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precisão')\n",
        "plt.title(f'Curva Precisão-Recall - {best_model_name}')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}